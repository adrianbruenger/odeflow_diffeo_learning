Considering an imitation learning problem, smooth, bijective maps (diffeomorphisms) can be utilized to guarantee stability for the learned dynamic system, by linking it to a stable \emph{latent} system. In this work, an \emph{inducing state} kernel method and a \emph{random Fourier feature} kernel approximation method, constructing such mappings, are presented. The kernel-based approaches are compared to the state-of-the-art \emph{neural ordinary differential equations} method. Evaluating the approaches on a handwriting dataset, results show that the methods are capable of learning dynamic systems reproducing serverly curved trajectories, closely rembling the demonstraions. Further, due to its simplicity, the \emph{random Fourier feature} method proves to be computationally faster then the state-of-the-art approach.
